{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c9kavJGTrWTE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9w-IfeBVr_88"
   },
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "max_features = 10000  # consider the top 10,000 most common words\n",
    "maxlen = 200  # cut off each review after 200 words\n",
    "batch_size = 32\n",
    "num_epochs = 3  # reduced number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awEMqluysDQS",
    "outputId": "f65ae2cf-9210-48a0-9068-59b487a001f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2YTcJw7sF5i",
    "outputId": "62a3b39a-1d27-45ae-e283-c6fff357495b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pad sequences to have the same length\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epe2h7zKsI56",
    "outputId": "4c0a182a-1c90-4885-a0b3-750daa5d4ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          1280000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25600)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25600)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 25601     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1305601 (4.98 MB)\n",
      "Trainable params: 1305601 (4.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UM2oohr3sLX2"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa_zRINDsOrA",
    "outputId": "e76d7f1f-4886-48bc-fe24-900e1e772a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3035 - accuracy: 0.8766\n",
      "Test loss: 0.3035024404525757\n",
      "Test accuracy: 0.8766400218009949\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQRTIRxBssDK",
    "outputId": "6c4a5647-bd9c-4dca-823a-323a677efc7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "Review: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? is the first of its kind in turkish cinema and it's way better than i expected those people who say it's neither scary nor funny have a point it's not all that great indeed but it must be kept in mind that everyone involved with the movie is rather amateur so it's basically a maiden voyage and comparing this one to other films such as the 1st class garbage propaganda this movie is pretty damn good br br one thing that must be said it deals with the ? ? life in turkey very realistically that's exactly how it goes the scenes that are meant to scare are somewhat cheap and ? most of them even if not all but that religion lesson scene made me laugh in tears and performs the best acting of this flick as a religion teacher br br it's not a waste of your time go and watch it you'll find it rather amusing especially if you know turkey enough to relate to turkish school lives\n",
      "Actual Label: 1\n",
      "Predicted Label (Probability): 0.058581293\n",
      "Predicted Label (Binary): 0\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset with word index\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Get the review and actual label\n",
    "review = x_test[101]\n",
    "actual_label = y_test[101]\n",
    "\n",
    "# Predict sentiment for the chosen review\n",
    "prediction = model.predict(np.expand_dims(review, axis=0))[0][0]\n",
    "rounded_prediction = round(prediction)\n",
    "\n",
    "# Print the review, actual label, and predicted label\n",
    "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in review])\n",
    "print(\"Review:\", decoded_review)\n",
    "print(\"Actual Label:\", actual_label)\n",
    "print(\"Predicted Label (Probability):\", prediction)\n",
    "print(\"Predicted Label (Binary):\", rounded_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CeqDpDl2tU-u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
